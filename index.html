<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lei Sun (å­™ç£Š)</title>
  
  <meta name="author" content="Lei Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’ª</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lei Sun (å­™ç£Š)</name>
              </p>
              <p>
                I am a post-doc researcher at <a href="https://insait.ai">INSAIT</a>, advised by Prof. <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl">Luc Van Gool</a>. and Dr. Danda Pani Paudel.
              <p>I earned my Ph.D. from Zhejiang University under the supervision of Prof. <a href="http://wangkaiwei.org/">Kaiwei Wang</a>.
                
                I was a visiting doctoral student at <a href="https://vision.ee.ethz.ch/">Robotics and Perception Group</a>, University of Zurich, under the supervision of Prof. <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>.
                I was also a visiting doctoral student at <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a>, 
                ETH Zurich under the supervision of Prof. <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a>. 

                Before that, I received my bachelor's degree from Beijing Institute of Technology.

                <p>
                  I'm interested in <strong>image/video generation</strong>, <strong>computational imaging</strong>, and <strong>event-based vision</strong>.
                </p>
              </p>
              <!-- <p>
                I am looking for post-doc positions or job opportunities related to event camera, low-level image process, and environment perception algorithms.
              </p> -->
              <p style="text-align:center">
                <a href="mailto:leo_sun@zju.edu.cn">Email</a> &nbsp/&nbsp
                <!-- <a href="data/sunlei_cv_2024mar.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=VLFyM50AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/lei-sun-ab6533246/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/ahupujr/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SunLei.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/SunLei_2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <div style="max-height: 200px; overflow-y: auto; border: 1px solid #ddd; padding: 10px;">
                  <ul>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Feb 2025:</span> 
                      I will serve as the organizer/co-organizer of 
                      <a href="https://codalab.lisn.upsaclay.fr/competitions/215608">Challenge On Image Denoising </a>, 
                      <a href="https://codalab.lisn.upsaclay.fr/competitions/21451">Challenge On Image Super-Resolution</a>, and 
                      <a href="https://codalab.lisn.upsaclay.fr/competitions/21702">Challenge on Real-World Face Restoration</a> in CVPR 2025 NTIRE workshop.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Feb 2025:</span> 
                      I am excited to organize the 
                      <a href="https://codalab.lisn.upsaclay.fr/competitions/21498">First Challenge on Event-Based Image Deblurring </a> in CVPR 2025 NTIRE workshop & Event-Based Vision workshop.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Feb 2025:</span> 
                      Two papers accepted to IEEE Trans. on Computational Imaging.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Jan 2025:</span> 
                      First-author paper accepted to T-PAMI.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Oct 2024:</span> 
                      One paper accepted to Optics & Laser Technology.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Nov 2024:</span> 
                      One paper accepted to Optics Express.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Oct 2024:</span> 
                      One paper accepted to Optics Express.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Aug 2024:</span> 
                      One paper accepted to T-IP.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Jul 2024:</span> 
                      One paper accepted to ECCV.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Jun 2024:</span> 
                      I am excited to have finished my <a href="./images/grad.png">Ph.D.</a>!
                      
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Apr 2024:</span> 
                      One paper accepted to IEEE Trans. on Computational Imaging.
                    </li>
                    <li style="margin-bottom: 10px;">
                      <span style="display:inline-block; width: 85px;">Previous:</span> ...
                    </li>
                  </ul>
                </div>
              </td>
            </tr> 
          </tbody>
        </table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <div style="max-height: 200px; overflow-y: auto; border: 1px solid #ddd; padding: 10px;">
                  <ul>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Feb 2025:</span> I will serve as the organizer/co-organizer of <a href="https://codalab.lisn.upsaclay.fr/competitions/215608">Challenge On Image Denoising </a>, <a href="https://codalab.lisn.upsaclay.fr/competitions/21451">Challenge On Image Super-Resolution</a>, and <a href="https://codalab.lisn.upsaclay.fr/competitions/21702">Challenge on Real-World Face Restoration</a> in CVPR 2025 NTIRE workshop.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Feb 2025:</span> I am excited to organize the <a href="https://codalab.lisn.upsaclay.fr/competitions/21498">First Challenge on Event-Based Image Deblurring </a> in CVPR 2025 NTIRE workshop & Event-Based Vision Workshop.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Feb 2025:</span> Two papers accepted to IEEE Trans. on Computational Imaging.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Jan 2025:</span> First-author paper accepted to T-PAMI.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Oct 2024:</span> One paper accepted to Optics & Laser Technology.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Oct 2024:</span> One paper accepted to Optics Express.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Aug 2024:</span> One paper accepted to T-IP.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Jul 2024:</span> One paper accepted to ECCV.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Jun 2024:</span> I am excited to have finished my Ph.D.!</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Apr 2024:</span> One paper accepted to IEEE Trans. on Computational Imaging.</li>
                    <li style="margin-bottom: 10px;"><span style="display:inline-block; width: 85px;">Previous:</span> ...</li>
                  </ul>
                </div>
              </td>
            </tr> 
          </tbody>
        </table> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Publications</heading>
            <p>
               First-author papers are <span class="highlight">highlighted</span>. * denotes equal contribution and &dagger; denotes corresponding author.
            </p>
          </td>
        </tr> 
      </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!--------------- Insert new paper here --------------->



          <!-- papers -->
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pami2025.png" alt="refid" width="200" height="130">
            </td>
            <td width="75%" valign="middle">
              <!-- <td style="padding:20px;width:75%;vertical-align:middle"> -->
                <!-- <a href="https://ahupujr.github.io/EFNet/"> -->
                  <papertitle>Event-Based Frame Interpolation with Ad-hoc Deblurring</papertitle>
                </a>
                <br>
                <strong>Lei Sun</strong>,
                <a href="https://danielgehrig18.github.io/">Daniel Gehrig</a>, 
                <a href="https://people.ee.ethz.ch/~csakarid">Christos Sakaridis</a>, 
                Mathias Gehrig,
                <br>
                <a href="https://jingyunliang.github.io">Jingyun Liang</a>, 
                Peng Sun,
                Zhiije Xu,
                <a href="http://wangkaiwei.org/">Kaiwei Wang</a>,
                <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl">Luc Van Gool</a>, and 
                <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>
                <br>
                <em>T-PAMI</em>, 2025
                <br>
                <!-- <a href="">project page</a> -->
                <!-- / -->
                <a href="https://github.com/AHupuJR/REFID"> codes</a>
                /
                <a href="https://arxiv.org/abs/2301.05191">arXiv</a>
                /
                <a href="">video</a>
                <p></p>
                <p>
                  Unified Event-based frame interpolation for both sharp frames and blurry frames with unsupervised domain adaption for unkown camera parameters.
                </p>
              <!-- </td> -->
              </td>
            </tr>
            <td>
            <br>
            </td>


            
          <!-- papers -->
          <tr onmouseout="evtemmap_stop()" onmouseover="evtemmap_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='evtemmap_image'>
                  <img src='images/timemap2.jpg' width="160"></div>
                <img src='images/timemap.jpg' width="160">
              </div>
              <script type="text/javascript">
                function evtemmap_start() {
                  document.getElementById('evtemmap_image').style.opacity = "1";
                }

                function evtemmap_stop() {
                  document.getElementById('evtemmap_image').style.opacity = "0";
                }
                evtemmap_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
                  <papertitle>Temporal-Mapping Photography for Event Cameras</papertitle>
                </a>
                <br>
                Yuhan Bao*, <strong>Lei Sun*&dagger;</strong>, Yuqin Ma, Diyang Gu, <a href="http://wangkaiwei.org/">Kaiwei Wang&dagger;</a>
                <br>
                <em>ECCV</em>, 2024
                <br>
                <a href="https://github.com/YuHanBaozju/EvTemMap/tree/EvTemMap_basic">Github</a>
                /
                <a href="https://arxiv.org/pdf/2403.06443.pdf">arXiv</a>
                
                <p></p>
                <p>
                  EvTemMap, pioneering work of event-to-image conversion, produces high-quality, high-grascale-resolution, high-dynamic-range image from events only.
                </p>
              <!-- </td> -->
              </td>
            </tr>
            <td>
            <br>
            </td>

            

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/sge.png" alt="event_af" width="160" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2403.07326.pdf">
                  <papertitle>SGE: Structured Light System Based on Gray Code with an Event Camera</papertitle>
                </a>
                <br>
                Xingyu Lu, <strong>Lei Sun&dagger; </strong>, Diyang Gu, Zhijiie Xu, <a href="http://wangkaiwei.org/">Kaiwei Wang&dagger;</a>
                <br>
                <em>Optics Express</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2403.07326.pdf">arXiv</a>
                <!-- <a href="">codes coming soon</a> -->
                <p> Event-based depth estimation for dynamic scenes, over 41 times faster in data acquisition than previous SOTA.</p>
              </td>
            </tr>



          <!-- papers -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/event_af.jpg" alt="event_af" width="160" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2303.08611.pdf">
                <papertitle>Improving Fast Auto-Focus with Event Polarity</papertitle>
              </a>
              <br>
              Yuhan Bao, <strong>Lei Sun</strong>, Yuqin Ma, Diyang Gu, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>
              <br>
              <em>Optics Express</em>, 2023 &nbsp <font color="red"><strong>(Editors' Pick)</strong></font>
              <br>
              <a href="">codes coming soon</a>
              <p>Event-assisted fast auto-focus algorithm. Precise focus with less than one depth of focus is achieved within 0.004 seconds.</p>
            </td>
          </tr>
          <!-- papers -->
          <td>
            <br>
            </td>


          <!-- papers -->
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/refid.jpg" alt="refid" width="160" height="90">
            </td>
            <td width="75%" valign="middle">
              <!-- <td style="padding:20px;width:75%;vertical-align:middle"> -->
                <!-- <a href="https://ahupujr.github.io/EFNet/"> -->
                  <papertitle>Event-Based Frame Interpolation with Ad-hoc Deblurring</papertitle>
                </a>
                <br>
                <strong>Lei Sun</strong>,
                <a href="https://people.ee.ethz.ch/~csakarid">Christos Sakaridis</a>, 
                <a href="https://jingyunliang.github.io">Jingyun Liang</a>, 
                Peng Sun,
                <a href="www.jiezhangcao.com">Jiezhang Cao</a>,
                <br>
                <a href="https://cszn.github.io/">Kai Zhang</a>,
                Qi Jiang,
                <a href="http://wangkaiwei.org/">Kaiwei Wang</a>,
                <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl">Luc Van Gool</a>
                <br>
                <em>CVPR</em>, 2023
                <br>
                <!-- <a href="">project page</a> -->
                <!-- / -->
                <a href="https://github.com/AHupuJR/REFID"> codes</a>
                /
                <a href="https://arxiv.org/pdf/2301.05191.pdf">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=pInRJ_O2kas&t=3s">Video</a>
                <p></p>
                <p>
                  SOTA unified Event-based frame interpolation for both sharp frames and blurry frames.
                </p>
              <!-- </td> -->
              </td>
            </tr>
            <td>
            <br>
            </td>

          <!-- papers -->
          <tr bgcolor="#ffffd0">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/Evdeblur_before.png" alt="evdeblur" width="160" height="120">
          </td>
          <td width="75%" valign="middle">
            <!-- <td style="padding:20px;width:75%;vertical-align:middle"> -->
              <a href="https://ahupujr.github.io/EFNet/">
                <papertitle>Event-Based Fusion for Motion Deblurring with Cross-modal Attention</papertitle>
              </a>
              <br>
              <strong>Lei Sun</strong>,
              <a href="https://people.ee.ethz.ch/~csakarid">Christos Sakaridis</a>, 
							<a href="https://jingyunliang.github.io">Jingyun Liang</a>, 
              Qi Jiang,
              <a href="https://yangkailun.com">Kailun Yang</a>,
              Peng Sun,
              <br>
              Yaozu Ye,
              <a href="http://wangkaiwei.org/">Kaiwei Wang</a>,
              <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl">Luc Van Gool</a>

              <br>
              <em>ECCV</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, rate:2.7%)</strong></font>
              <br>
              <a href="https://ahupujr.github.io/EFNet/">project page</a>
              /
              <a href="https://github.com/AHupuJR/EFNet">codes</a>
              /
              <a href="https://arxiv.org/abs/2112.00167">arXiv</a>
              <p></p>
              <p>
                We present a novel event-based deblurring method that improves the previous state-of-the-art by 2.47 dB.
              </p>
            <!-- </td> -->
            </td>
          </tr>

          <td>
          <br>
          </td>
      
      <!-- papers -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/ACI.jpg" alt="aci" width="160" height="60">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2206.06070">
            <papertitle>Annular computational imaging: Capture clear panoramic images through simple lens</papertitle>
          </a>
          <br>
          Qi Jiang, Hao Shi, <strong>Lei Sun</strong>, Shaohua Gao, <a href="https://yangkailun.com">Kailun Yang</a>, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>
          <br>
          <em>IEEE Transactions on Computational Imaging</em>, 2022
          <br>
          <a href="https://github.com/zju-jiangqi/ACI-PI2RNet">codes</a>
          <p>Annular Computational Imaging (ACI) framework to break the optical limit of light-weight Panoramic Annular Lens design.  </p>
        </td>
      </tr>
      <!-- papers -->

      <td>
        <br>
        </td>


      <!-- papers -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/event_hpe.png" alt="event_hpe" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2206.04511">
            <papertitle>Efficient Human Pose Estimation via 3D Event Point Cloud</papertitle>
          </a>
          <br>
          Jiaan Chen*, Hao Shi*, Yaozu Ye, <a href="https://yangkailun.com">Kailun Yang</a>, <strong>Lei Sun</strong>, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>
          <br>
          <em>3DV</em>, 2022
          <br>
          <a href="https://github.com/MasterHow/EventPointPose">codes</a>
          <p>A lightweight event-based human pose estimation model by processing events as point cloud.</p>
        </td>
      </tr>
      <!-- papers -->

            <td>
            <br>
            </td>


      <!-- papers -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/PALrestore.jpg" alt="PALrestore" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-17-29940&id=487064">
            <papertitle>Compact and lightweight panoramic annular lens for computer vision tasks</papertitle>
          </a>
          <br>
          Shaohua Gao, <strong>Lei Sun</strong>, Qi Jiang, Hao Shi, Jia Wang, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>, Jian Bai
          <br>
          <em>Optics Express</em>, 2022 &nbsp <font color="red"><strong>(Editors' Pick)</strong></font>
          <p>Designing a lightweight panoramic annular lens with physical-based image enhancement model.</p>
        </td>
      </tr>
      <!-- papers -->

            <td>
            <br>
            </td>

      <!-- papers -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/Wangjia_seg.png" alt="Wangjia_seg" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://yangkailun.com/publications/oe2022_jia.pdf">
            <papertitle>High-performance panoramic annular lens design for real-time semantic segmentation on aerial imagery</papertitle>
          </a>
          <br>
          Jia Wang, <a href="https://yangkailun.com">Kailun Yang</a>, Shaohua Gao, <strong>Lei Sun</strong>, Chenxi Zhu, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>, Jian Bai
          <br>
          <em>Optical Engineering</em> 61 (3), 035101, 2022
          <p>Designing a panoramic annular lens (PAL) system with 4K high resolution for aerial image segmentation.</p>
        </td>
      </tr>
      <!-- papers -->

            <td>
            <br>
            </td>

      <!-- papers -->
      <tr bgcolor="#ffffd0"> 
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/Aerial_pass.png" alt="Aerial_pass" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2105.07209.pdf">
            <papertitle>Aerial-PASS: panoramic annular scene segmentation in drone videos</papertitle>
          </a>
          <br>
          <strong>Lei Sun</strong>, Jia Wang, <a href="https://yangkailun.com">Kailun Yang</a>, Kaikai Wu, Xiangdong Zhou, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>, Jian Bai
          <br>
          <em>European Conference on Mobile Robots (ECMR)</em> 2021
          <p>Designing a specific semantic segmentation for aerial panoramic images.</p>
        </td>
      </tr>
      <!-- papers -->

      <td>
        <br>
        </td>

      <!-- papers -->
      <tr bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/RFNet.png" alt="RFNet" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2002.10570.pdf">
            <papertitle>Real-time fusion network for RGB-D semantic segmentation incorporating unexpected obstacle detection for road-driving images</papertitle>
          </a>
          <br>
          <strong>Lei Sun</strong>, <a href="https://yangkailun.com">Kailun Yang</a>, Xinxin Hu, Weijian Hu, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>
          <br>
          <em>IEEE Robotics and Automation Letters and IROS</em>, 2020
          <br>
          <a href="https://github.com/AHupuJR/EFNet">codes</a>
          <br>
          <p>A real-time RGB-D fusion semantic segmentation framework with small obstacle detection.</p>
        </td>
      </tr>
      <!-- papers -->

      <td>
        <br>
        </td>

      <!-- papers -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/weijian_vip.png" alt="weijian_vip" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=VLFyM50AAAAJ&citation_for_view=VLFyM50AAAAJ:W7OEmFMy1HYC">
            <papertitle>A comparative study in real-time scene sonification for visually impaired people</papertitle>
          </a>
          <br>
          Weijian Hu, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>, <a href="https://yangkailun.com">Kailun Yang</a>, Ruiqi Cheng, Yaozu Ye, <strong>Lei Sun</strong>, Zhijie Xu
          <br>
          <em>Sensors</em> 20 (11), 3222, 2020
          <p>Propose three different auditory-based interaction methods which convey raw depth images,
            obstacle information and path information respectively to visually impaired people.</p>
        </td>
      </tr>
      <!-- papers -->
      <td>
        <br>
        </td>
      <!-- papers -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/Yichen_localization.png" alt="yichen_loal" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=VLFyM50AAAAJ&citation_for_view=VLFyM50AAAAJ:WF5omc3nYNoC">
            <papertitle>A panoramic localizer based on coarse-to-fine descriptors for navigation assistance</papertitle>
          </a>
          <br>
          Yicheng Fang,  <a href="https://yangkailun.com">Kailun Yang</a>, Ruiqi Cheng, <strong>Lei Sun</strong>, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>
          <br>
          <em>Sensors</em> 20 (15), 4177, 2020
          <p>Propose a panoramic
            localizer, which is based on coarse-to-fine descriptors, leveraging panoramas for omnidirectional
            perception and sufficient FoV up to 360â—¦.</p>
        </td>
      </tr>
      <!-- papers -->
      <td>
        <br>
        </td>

      <!-- papers -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/yyz_journal.png" alt="yyz_journal" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://iopscience.iop.org/article/10.1088/1742-6596/1229/1/012026/pdf">
            <papertitle>A wearable vision-to-audio sensory substitution device for blind assistance and the correlated neural substrates</papertitle>
          </a>
          <br>
          Yaozu Ye, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>, Weijian Hu, Huabing Li, <a href="https://yangkailun.com">Kailun Yang</a>, <strong>Lei Sun</strong>, Zuobing Chen
          <br>
          <em>Journal of Physics: Conference Series</em> 1229 (1), 012026, 2019
          <p>Develop a wearable system to transform the
            spatial information captured by camera into a voice description and fed it back to blind users.
          </p>
        </td>
      </tr>
      <!-- papers -->
            <td>
            <br>
            </td>
      <!-- papers -->
      <tr bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/day2night.png" alt="day2night" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/1908.05868.pdf">
            <papertitle>See Clearer at Night: Towards Robust Nighttime Semantic Segmentation through Day-Night Image Conversion</papertitle>
          </a>
          <br>
          <strong>Lei Sun</strong>, <a href="https://yangkailun.com">Kailun Yang</a>, <a href="http://wangkaiwei.org/">Kaiwei Wang</a>, Kaite Xiang
          <br>
          <em>Artificial Intelligence and Machine Learning in Defense Applications, SPIE</em> 1229 (1), 012026, 2019
          <p>Propose a
            framework to alleviate the accuracy decline when semantic segmentation is taken to adverse conditions by using
            Generative Adversarial Networks (GANs).
          </p>
        </td>
      </tr>
      <!-- papers -->

      <td>
        <br>
        </td>

        </tbody></table>

        <!-- Service -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
              <p>
                Co-organizer of <a href="https://www.cvlai.net/ntire/2025/">CVPR 2025 New Trends in Image Restoration and Enhancement (NTIRE) workshop</a>.
              </p>
              <p>
                Organizer of the <a href="https://codalab.lisn.upsaclay.fr/competitions/21498">First Challenge On Event-based Image Deblurring</a> in CVPR 2025  <a href="https://www.cvlai.net/ntire/2025/">NTIRE workshop</a> & <a href="https://tub-rip.github.io/eventvision2025/#3-event-based-image-deblurring-challenge">Event-Based Vision workshop</a>.
              </p>
              <p>
                Organizer of the <a href="https://codalab.lisn.upsaclay.fr/competitions/215608">Challenge On Image Denoising</a> in CVPR 2025  <a href="https://www.cvlai.net/ntire/2025/">NTIRE workshop</a>.
              </p>
              <p>
                Co-organizer of the <a href="https://codalab.lisn.upsaclay.fr/competitions/21451">Challenge On Image Super-Resolution</a>, and <a href="https://codalab.lisn.upsaclay.fr/competitions/21702">Challenge on Real-World Face Restoration</a> in CVPR 2025  <a href="https://www.cvlai.net/ntire/2025/">NTIRE workshop</a>.
              </p>

              <p>
                Reviewer for CVPR, ICCV, ECCV, AAAI, WACV, IJCV, TPAMI, TIP, RA-L, CVIU.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					

        <!-- Talk -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Talks</heading>
              <p>
                [<a href="https://www.youtube.com/watch?v=2SijuyeC05w"> Video </a>]
                Invited talk on event-based image deblurring in <a href="https://mipi-challenge.org/MIPI2022/">Mobile Intelligent Photography and Imaging (MIPI) </a>.
                
              </p>

              <p>
                <!-- [<a href="https://www.youtube.com/watch?v=2SijuyeC05w"> Video </a>] -->
                Invited talk in <a href="https://sites.google.com/view/iv2022-bsl-workshop/home">IV 2022 BSL Workshop</a>.
                
              </p>

            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					



        <!-- Education -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education & Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					


          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/insait_logo.png" alt="rpg" width="140" height="40">
            </td>
            <td width="75%" valign="center">
              Post-doc researcher
              <br>
              <a href="https://insait.ai">INSAIT</a>
              <br>
              Sofia, Bulgaria
              <br>
              Oct. 2024 ~ Now
              <br>
              Supervisor: Prof. <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl">Luc Van Gool</a>
              <br>
              Co-supervisor: Dr. Danda Pani Paudel
            </td>
          </tr>


          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/rpg.jpg" alt="rpg" width="140" height="80">
            </td>
            <td width="75%" valign="center">
              Visiting doctoral student
              <br>
              <a href="https://rpg.ifi.uzh.ch/index.html">Robotics and Perception Group</a>, <a href="https://www.uzh.ch/en.html">University of Zurich</a> and <a href="https://ethz.ch/en.html">ETH Zurich</a>
              <br>
              Zurich, Switzerland
              <br>
              Jan. 2023 ~ Sep. 2023
              <br>
              Supervisor: Prof. <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>
            </td>
          </tr>



          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/cvl.gif" alt="eth" width="140" height="80">
            </td>
            <td width="75%" valign="center">
              Visiting doctoral student
              <br>
              <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>
              <br>
              Zurich, Switzerland
              <br>
              Sep. 2021 ~ Jan. 2023
              <br>
              Supervisor: Prof. <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl">Luc Van Gool</a>
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/zju_logo.png" alt="zju" width="80" height="80">
            </td>
            <td width="75%" valign="center">
              Doctoral student
              <br>
              College of Optical Science and Enginnering, <a href="https://www.zju.edu.cn/">Zhejiang University</a>, Hangzhou, China
              <br>
              Sep. 2018 ~ Jun.2024 (expected)
              <br>
              Supervisor: Prof. <a href="http://wangkaiwei.org/">Kaiwei Wang</a>
            </td>
          </tr> -->
					<tr>
            <td style="padding:10px; width:25%; text-align:left; vertical-align:middle;">
                <img src="images/zju_logo.png" alt="zju" width="80" height="80">
            </td>
            <td width="75%" valign="middle">
                Doctor of Philosophy
                <br>
                <a href="https://www.zju.edu.cn/">Zhejiang University</a>
                <br>
                Hangzhou, China
                <br>
                Sep. 2018 ~ Jun. 2024
                <br>
                Supervisor: Prof. <a href="http://wangkaiwei.org/">Kaiwei Wang</a>
            </td>
        </tr>
        
        <tr>
            <td style="padding:10px; width:25%; text-align:left; vertical-align:middle;">
                <img src="images/bit_logo.png" alt="bit" width="80" height="80">
            </td>
            <td width="75%" valign="middle">
                Bachelor's degree
                <br>
                <a href="https://www.bit.edu.cn/">Beijing Institute of Technology</a>
                <br>
                Beijing, China
                <br>
                Sep. 2014 ~ Jun. 2018
            </td>
        </tr>
<!-- Education -->

          <!-- <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr> -->
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
